"""
Wave-Based Vectorized MCTS Integration Test
==========================================

This test validates the integration of key quantum-MCTS components:
1. Wave-based vectorized processing (3072-path waves)
2. Minhash similarity detection for path interference
3. Phase-kicked policy with adaptive parameters
4. Pragmatic quantum corrections based on research insights

Tests the complete pipeline from path extraction through quantum corrections
to final action selection, ensuring all components work together effectively.
"""

import torch
import numpy as np
import time
import math
from typing import Dict, List, Tuple, Optional, Any
import logging

# Set up imports
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from mcts.quantum.pragmatic_quantum_mcts import (
    create_pragmatic_quantum_mcts, create_conservative_quantum_mcts,
    PragmaticQuantumConfig, SearchPhase
)
from mcts.quantum.selective_quantum_optimized import create_selective_quantum_mcts
from mcts.quantum.unified_config import UnifiedQuantumConfig

# Try to import wave-based and minhash components
try:
    from mcts.quantum.research.wave_quantum_mcts import WaveQuantumMCTS, PathExtractor
    WAVE_AVAILABLE = True
except ImportError:
    WAVE_AVAILABLE = False
    print("Wave quantum MCTS not available - using fallback implementations")

try:
    from mcts.quantum.interference_gpu import MinHashInterference
    MINHASH_AVAILABLE = True
except ImportError:
    MINHASH_AVAILABLE = False
    print("MinHash interference not available - using fallback implementations")

logger = logging.getLogger(__name__)

class WaveVectorizedIntegrationTester:
    """
    Comprehensive tester for wave-based vectorized MCTS integration
    
    Tests all key components working together:
    - 3072-path wave processing
    - Minhash-based path similarity detection
    - Phase-kicked adaptive policy
    - Pragmatic quantum corrections
    """
    
    def __init__(self, device: str = 'cpu'):
        self.device = torch.device(device)
        
        # Initialize quantum MCTS variants
        self.pragmatic_mcts = create_pragmatic_quantum_mcts(device=device)
        self.conservative_mcts = create_conservative_quantum_mcts(device=device)
        self.selective_mcts = create_selective_quantum_mcts(device=device, enable_cuda_kernels=False)
        
        # Initialize wave processing if available
        if WAVE_AVAILABLE:
            self.wave_mcts = WaveQuantumMCTS(device=device)
            self.path_extractor = PathExtractor()
        else:
            self.wave_mcts = None
            self.path_extractor = None
        
        # Initialize minhash interference if available
        if MINHASH_AVAILABLE:
            self.minhash = MinHashInterference(device=device)
        else:
            self.minhash = None
        
        # Test configuration
        self.wave_size = 3072  # Optimal wave size for GPU
        self.num_actions = 30  # Typical for Gomoku
        
        logger.info(f"WaveVectorizedIntegrationTester initialized on {device}")
        logger.info(f"  Wave processing available: {WAVE_AVAILABLE}")
        logger.info(f"  MinHash available: {MINHASH_AVAILABLE}")
    
    def test_wave_processing_performance(self) -> Dict[str, Any]:
        """Test wave-based processing performance and correctness"""
        print("üåä Testing Wave-Based Processing...")
        
        # Generate test data for wave processing
        batch_size = self.wave_size
        q_values_batch = torch.randn(batch_size, self.num_actions, device=self.device) * 0.3
        visit_counts_batch = torch.randint(1, 50, (batch_size, self.num_actions), 
                                         dtype=torch.float32, device=self.device)
        priors_batch = torch.softmax(torch.randn(batch_size, self.num_actions, device=self.device), dim=-1)
        parent_visits_batch = torch.sum(visit_counts_batch, dim=-1)
        
        # Test pragmatic quantum MCTS batch processing
        start_time = time.time()
        
        pragmatic_scores = self.pragmatic_mcts.batch_compute_enhanced_ucb(
            q_values_batch, visit_counts_batch, priors_batch, 
            parent_visits_batch, simulation_count=1000
        )
        
        pragmatic_time = time.time() - start_time
        
        # Test selective quantum MCTS for comparison
        start_time = time.time()
        
        selective_scores_list = []
        for i in range(batch_size):
            scores = self.selective_mcts.apply_selective_quantum(
                q_values_batch[i], visit_counts_batch[i], priors_batch[i],
                parent_visits=parent_visits_batch[i].item(), simulation_count=1000
            )
            selective_scores_list.append(scores)
        
        selective_scores = torch.stack(selective_scores_list)
        selective_time = time.time() - start_time
        
        # Performance analysis
        throughput_pragmatic = batch_size / pragmatic_time  # paths/second
        throughput_selective = batch_size / selective_time   # paths/second
        speedup = throughput_pragmatic / throughput_selective
        
        # Correctness analysis
        score_difference = torch.mean(torch.abs(pragmatic_scores - selective_scores))
        relative_difference = score_difference / (torch.mean(torch.abs(selective_scores)) + 1e-8)
        
        results = {
            'wave_processing_working': True,
            'batch_size': batch_size,
            'pragmatic_time_seconds': pragmatic_time,
            'selective_time_seconds': selective_time,
            'throughput_pragmatic_paths_per_sec': throughput_pragmatic,
            'throughput_selective_paths_per_sec': throughput_selective,
            'vectorization_speedup': speedup,
            'score_difference': score_difference.item(),
            'relative_difference': relative_difference.item(),
            'correctness_ok': relative_difference < 0.2  # 20% tolerance for different algorithms
        }
        
        print(f"  ‚úì Processed {batch_size} paths")
        print(f"  ‚úì Pragmatic: {throughput_pragmatic:.0f} paths/sec")
        print(f"  ‚úì Selective: {throughput_selective:.0f} paths/sec")
        print(f"  ‚úì Speedup: {speedup:.2f}x")
        print(f"  ‚úì Relative difference: {relative_difference:.4f}")
        
        return results
    
    def test_minhash_similarity_detection(self) -> Dict[str, Any]:
        """Test MinHash-based path similarity detection"""
        print("üîç Testing MinHash Similarity Detection...")
        
        if not MINHASH_AVAILABLE:
            print("  ‚ö†Ô∏è  MinHash not available - using simplified similarity test")
            return self._test_simplified_similarity()
        
        # Generate test paths with known similarities
        num_paths = 1000
        path_length = 20
        
        # Create paths with controlled similarity
        base_path = torch.randint(0, self.num_actions, (path_length,), device=self.device)
        
        similar_paths = []
        dissimilar_paths = []
        
        # Generate similar paths (80% overlap)
        for i in range(num_paths // 2):
            path = base_path.clone()
            # Modify 20% of the path
            modify_indices = torch.randperm(path_length)[:path_length // 5]
            path[modify_indices] = torch.randint(0, self.num_actions, (len(modify_indices),), device=self.device)
            similar_paths.append(path)
        
        # Generate dissimilar paths (random)
        for i in range(num_paths // 2):
            path = torch.randint(0, self.num_actions, (path_length,), device=self.device)
            dissimilar_paths.append(path)
        
        all_paths = similar_paths + dissimilar_paths
        path_tensor = torch.stack(all_paths)
        
        # Test MinHash similarity detection
        start_time = time.time()
        
        similarity_matrix = self.minhash.compute_similarities(path_tensor)
        
        minhash_time = time.time() - start_time
        
        # Analyze results
        # Similar paths should have higher similarity to base path
        base_similarities = similarity_matrix[0, :]  # Similarities to base path
        similar_similarities = base_similarities[1:num_paths//2 + 1]  # Similar paths
        dissimilar_similarities = base_similarities[num_paths//2 + 1:]  # Dissimilar paths
        
        avg_similar_similarity = torch.mean(similar_similarities)
        avg_dissimilar_similarity = torch.mean(dissimilar_similarities)
        
        discrimination_ratio = avg_similar_similarity / (avg_dissimilar_similarity + 1e-8)
        
        results = {
            'minhash_working': True,
            'num_paths': num_paths,
            'processing_time_seconds': minhash_time,
            'paths_per_second': num_paths / minhash_time,
            'avg_similar_similarity': avg_similar_similarity.item(),
            'avg_dissimilar_similarity': avg_dissimilar_similarity.item(),
            'discrimination_ratio': discrimination_ratio.item(),
            'discrimination_ok': discrimination_ratio > 1.5  # Similar paths should be 1.5x more similar
        }
        
        print(f"  ‚úì Processed {num_paths} paths in {minhash_time:.3f}s")
        print(f"  ‚úì Throughput: {num_paths/minhash_time:.0f} paths/sec")
        print(f"  ‚úì Similar path similarity: {avg_similar_similarity:.3f}")
        print(f"  ‚úì Dissimilar path similarity: {avg_dissimilar_similarity:.3f}")
        print(f"  ‚úì Discrimination ratio: {discrimination_ratio:.2f}")
        
        return results
    
    def test_phase_kicked_policy(self) -> Dict[str, Any]:
        """Test phase-kicked adaptive policy mechanism"""
        print("üéØ Testing Phase-Kicked Policy...")
        
        # Test phase detection across different scenarios
        phase_test_cases = [
            # (visit_counts, expected_phase)
            (torch.tensor([50., 1., 1., 1., 1.]), SearchPhase.EXPLOITATION),  # Focused
            (torch.tensor([5., 5., 5., 5., 5.]), SearchPhase.CRITICAL),        # Balanced  
            (torch.tensor([2., 2., 2., 2., 2.]), SearchPhase.EXPLORATION),     # Uniform
        ]
        
        phase_detection_results = []
        
        for i, (visit_counts, expected_phase) in enumerate(phase_test_cases):
            detected_phase = self.pragmatic_mcts.phase_detector.detect_phase(visit_counts)
            phase_correct = detected_phase == expected_phase
            
            # Test adaptive c_puct
            adaptive_c_puct = self.pragmatic_mcts._get_adaptive_c_puct(detected_phase)
            base_c_puct = self.pragmatic_mcts.config.base_c_puct
            
            phase_detection_results.append({
                'case': i,
                'visit_pattern': visit_counts.tolist(),
                'expected_phase': expected_phase.value,
                'detected_phase': detected_phase.value,
                'phase_correct': phase_correct,
                'adaptive_c_puct': adaptive_c_puct,
                'c_puct_ratio': adaptive_c_puct / base_c_puct
            })
            
            print(f"  ‚úì Case {i}: {visit_counts.tolist()} ‚Üí {detected_phase.value} (c_puct={adaptive_c_puct:.2f})")
        
        # Test power-law annealing
        annealing_test_points = [10, 100, 1000, 10000]
        annealing_results = []
        
        for sim_count in annealing_test_points:
            temperature = self.pragmatic_mcts.annealer.get_temperature(sim_count)
            annealing_results.append({
                'simulation_count': sim_count,
                'temperature': temperature
            })
        
        # Check that temperature decreases (annealing)
        temperatures = [r['temperature'] for r in annealing_results]
        annealing_working = all(temperatures[i] >= temperatures[i+1] for i in range(len(temperatures)-1))
        
        results = {
            'phase_detection_working': all(r['phase_correct'] for r in phase_detection_results),
            'phase_detection_cases': phase_detection_results,
            'annealing_working': annealing_working,
            'annealing_results': annealing_results,
            'adaptive_c_puct_working': any(r['c_puct_ratio'] != 1.0 for r in phase_detection_results)
        }
        
        print(f"  ‚úì Phase detection: {results['phase_detection_working']}")
        print(f"  ‚úì Annealing: {results['annealing_working']}")
        print(f"  ‚úì Adaptive c_puct: {results['adaptive_c_puct_working']}")
        
        return results
    
    def test_quantum_corrections_integration(self) -> Dict[str, Any]:
        """Test integration of quantum corrections with wave processing"""
        print("‚öõÔ∏è  Testing Quantum Corrections Integration...")
        
        # Test scenarios with different visit patterns
        test_scenarios = [
            {
                'name': 'early_exploration',
                'visit_counts': torch.tensor([1., 2., 1., 3., 1.]),
                'simulation_count': 100,
                'expect_quantum_bonus': True
            },
            {
                'name': 'mature_exploitation', 
                'visit_counts': torch.tensor([100., 80., 90., 70., 85.]),
                'simulation_count': 5000,
                'expect_quantum_bonus': False
            },
            {
                'name': 'mixed_exploration',
                'visit_counts': torch.tensor([50., 2., 45., 3., 40.]),
                'simulation_count': 2000,
                'expect_quantum_bonus': True  # Some low-visit nodes
            }
        ]
        
        integration_results = []
        
        for scenario in test_scenarios:
            visit_counts = scenario['visit_counts']
            num_actions = len(visit_counts)
            
            # Generate test data
            q_values = torch.randn(num_actions) * 0.2
            priors = torch.softmax(torch.randn(num_actions), dim=0)
            parent_visits = int(torch.sum(visit_counts).item())
            
            # Compute quantum-enhanced scores
            quantum_scores = self.pragmatic_mcts.compute_enhanced_ucb_scores(
                q_values, visit_counts, priors, parent_visits, scenario['simulation_count']
            )
            
            # Compute classical baseline (using selective MCTS without quantum)
            classical_scores = self.selective_mcts._classical_v5_vectorized(
                q_values, visit_counts, priors, float(parent_visits)
            )
            
            # Analyze quantum effect
            quantum_difference = quantum_scores - classical_scores
            max_quantum_boost = torch.max(quantum_difference)
            avg_quantum_boost = torch.mean(quantum_difference)
            
            # Check if quantum bonuses are applied where expected
            low_visit_mask = visit_counts < 50
            quantum_bonus_on_low_visits = torch.any(quantum_difference[low_visit_mask] > 0.01) if torch.any(low_visit_mask) else False
            
            scenario_result = {
                'scenario_name': scenario['name'],
                'max_quantum_boost': max_quantum_boost.item(),
                'avg_quantum_boost': avg_quantum_boost.item(),
                'quantum_bonus_on_low_visits': quantum_bonus_on_low_visits,
                'expectation_met': (quantum_bonus_on_low_visits == scenario['expect_quantum_bonus']),
                'visit_pattern': visit_counts.tolist(),
                'simulation_count': scenario['simulation_count']
            }
            
            integration_results.append(scenario_result)
            
            print(f"  ‚úì {scenario['name']}: max_boost={max_quantum_boost:.4f}, "
                  f"low_visit_bonus={quantum_bonus_on_low_visits}")
        
        # Overall integration assessment
        integration_working = all(r['expectation_met'] for r in integration_results)
        
        results = {
            'quantum_integration_working': integration_working,
            'scenario_results': integration_results,
            'pragmatic_quantum_enabled': self.pragmatic_mcts.config.enable_quantum_bonus,
            'phase_adaptation_enabled': self.pragmatic_mcts.config.enable_phase_adaptation
        }
        
        return results
    
    def test_end_to_end_performance(self) -> Dict[str, Any]:
        """Test end-to-end performance of integrated system"""
        print("üèÅ Testing End-to-End Performance...")
        
        # Large-scale performance test
        num_waves = 10
        wave_size = self.wave_size
        total_paths = num_waves * wave_size
        
        # Generate realistic test data
        q_values_full = torch.randn(total_paths, self.num_actions, device=self.device) * 0.3
        visit_counts_full = torch.randint(1, 100, (total_paths, self.num_actions), 
                                        dtype=torch.float32, device=self.device)
        priors_full = torch.softmax(torch.randn(total_paths, self.num_actions, device=self.device), dim=-1)
        parent_visits_full = torch.sum(visit_counts_full, dim=-1)
        
        # Test pragmatic quantum MCTS
        start_time = time.time()
        
        quantum_results = []
        for wave_idx in range(num_waves):
            start_idx = wave_idx * wave_size
            end_idx = start_idx + wave_size
            
            wave_scores = self.pragmatic_mcts.batch_compute_enhanced_ucb(
                q_values_full[start_idx:end_idx],
                visit_counts_full[start_idx:end_idx], 
                priors_full[start_idx:end_idx],
                parent_visits_full[start_idx:end_idx],
                simulation_count=wave_idx * 100
            )
            quantum_results.append(wave_scores)
        
        quantum_time = time.time() - start_time
        
        # Test classical baseline
        start_time = time.time()
        
        classical_results = []
        for wave_idx in range(num_waves):
            start_idx = wave_idx * wave_size
            end_idx = start_idx + wave_size
            
            wave_scores = []
            for i in range(wave_size):
                scores = self.selective_mcts._classical_v5_vectorized(
                    q_values_full[start_idx + i],
                    visit_counts_full[start_idx + i],
                    priors_full[start_idx + i],
                    parent_visits_full[start_idx + i].item()
                )
                wave_scores.append(scores)
            
            classical_results.append(torch.stack(wave_scores))
        
        classical_time = time.time() - start_time
        
        # Performance analysis
        quantum_throughput = total_paths / quantum_time
        classical_throughput = total_paths / classical_time
        overhead_ratio = quantum_time / classical_time
        
        # Quality analysis (simplified)
        quantum_scores_all = torch.cat(quantum_results, dim=0)
        classical_scores_all = torch.cat(classical_results, dim=0)
        
        score_improvement = torch.mean(quantum_scores_all - classical_scores_all)
        
        # Statistics from pragmatic MCTS
        performance_stats = self.pragmatic_mcts.get_performance_stats()
        
        results = {
            'end_to_end_working': True,
            'total_paths_processed': total_paths,
            'quantum_time_seconds': quantum_time,
            'classical_time_seconds': classical_time,
            'quantum_throughput_paths_per_sec': quantum_throughput,
            'classical_throughput_paths_per_sec': classical_throughput,
            'performance_overhead_ratio': overhead_ratio,
            'overhead_under_target': overhead_ratio < 1.5,  # Target < 1.5x overhead
            'average_score_improvement': score_improvement.item(),
            'performance_stats': performance_stats
        }
        
        print(f"  ‚úì Processed {total_paths} paths total")
        print(f"  ‚úì Quantum: {quantum_throughput:.0f} paths/sec")
        print(f"  ‚úì Classical: {classical_throughput:.0f} paths/sec")
        print(f"  ‚úì Overhead: {overhead_ratio:.2f}x")
        print(f"  ‚úì Score improvement: {score_improvement:.6f}")
        
        return results
    
    def _test_simplified_similarity(self) -> Dict[str, Any]:
        """Simplified similarity test when MinHash is not available"""
        # Simple Q-value based similarity test
        num_nodes = 100
        q_values = torch.randn(num_nodes, device=self.device)
        visit_counts = torch.randint(1, 50, (num_nodes,), dtype=torch.float32, device=self.device)
        
        similarity_scores = self.pragmatic_mcts.correlation_analyzer.get_exploration_priorities(
            q_values, visit_counts
        )
        
        return {
            'simplified_similarity_working': torch.all(torch.isfinite(similarity_scores)),
            'num_nodes': num_nodes,
            'avg_priority': torch.mean(similarity_scores).item()
        }
    
    def run_comprehensive_integration_test(self) -> Dict[str, Any]:
        """Run comprehensive integration test of all components"""
        print("Wave-Based Vectorized MCTS Integration Test")
        print("=" * 50)
        
        results = {}
        
        try:
            # Test individual components
            results['wave_processing'] = self.test_wave_processing_performance()
            results['minhash_similarity'] = self.test_minhash_similarity_detection()
            results['phase_kicked_policy'] = self.test_phase_kicked_policy()
            results['quantum_corrections'] = self.test_quantum_corrections_integration()
            results['end_to_end'] = self.test_end_to_end_performance()
            
            # Overall assessment
            component_results = [
                results['wave_processing']['wave_processing_working'],
                results['minhash_similarity'].get('minhash_working', True),
                results['phase_kicked_policy']['phase_detection_working'],
                results['quantum_corrections']['quantum_integration_working'],
                results['end_to_end']['end_to_end_working']
            ]
            
            overall_success = all(component_results)
            success_count = sum(component_results)
            
            results['overall'] = {
                'integration_successful': overall_success,
                'components_working': success_count,
                'total_components': len(component_results),
                'success_rate': success_count / len(component_results)
            }
            
        except Exception as e:
            results['error'] = str(e)
            results['overall'] = {
                'integration_successful': False,
                'error': str(e)
            }
        
        # Print summary
        print("\n" + "=" * 50)
        if results.get('overall', {}).get('integration_successful', False):
            print("‚úÖ INTEGRATION SUCCESSFUL")
            overall = results['overall']
            print(f"All {overall.get('components_working', 0)}/{overall.get('total_components', 0)} components working")
            print("\nüéâ Wave-based vectorized MCTS with quantum corrections is fully operational!")
        else:
            print("‚ùå INTEGRATION ISSUES DETECTED")
            overall = results.get('overall', {})
            print(f"Working: {overall.get('components_working', 0)}/{overall.get('total_components', 0)} components")
            if 'error' in results:
                print(f"Error: {results['error']}")
        
        return results

def main():
    """Run the comprehensive integration test"""
    device = 'cpu'  # Use CPU for compatibility
    
    tester = WaveVectorizedIntegrationTester(device=device)
    results = tester.run_comprehensive_integration_test()
    
    # Detailed results summary
    print(f"\nüìä DETAILED RESULTS SUMMARY:")
    if 'wave_processing' in results:
        wp = results['wave_processing']
        print(f"Wave Processing: {wp['vectorization_speedup']:.2f}x speedup, {wp['throughput_pragmatic_paths_per_sec']:.0f} paths/sec")
    
    if 'phase_kicked_policy' in results:
        pkp = results['phase_kicked_policy']
        print(f"Phase-Kicked Policy: Detection={pkp['phase_detection_working']}, Annealing={pkp['annealing_working']}")
    
    if 'end_to_end' in results:
        e2e = results['end_to_end']
        print(f"End-to-End: {e2e['overhead_ratio']:.2f}x overhead, {e2e['quantum_throughput_paths_per_sec']:.0f} paths/sec")
    
    return results['overall']['integration_successful']

if __name__ == "__main__":
    success = main()