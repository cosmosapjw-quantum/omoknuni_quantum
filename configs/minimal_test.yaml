# Minimal Test Configuration
# For quick testing and debugging - not for actual training

experiment_name: test_run
seed: 42
log_level: DEBUG
num_iterations: 5

game:
  game_type: gomoku
  board_size: 9  # Smaller board for testing

mcts:
  # Minimal settings for speed
  num_simulations: 100
  c_puct: 1.0
  dirichlet_alpha: 0.3
  dirichlet_epsilon: 0.25
  
  # Small batches for testing
  min_wave_size: 64
  max_wave_size: 128
  batch_size: 64
  
  # Virtual loss
  virtual_loss: 0.0
  enable_virtual_loss: false
  
  # Minimal memory
  memory_pool_size_mb: 512
  max_tree_nodes: 10000
  tree_reuse: false
  
  # Quantum features disabled
  enable_quantum: false
  
  # Progressive widening parameters for lazy expansion
  progressive_widening_alpha: 0.5  # k = alpha * sqrt(n) - controls expansion rate
  progressive_widening_base: 10.0  # Minimum children to expand at each node
  
  # Device - still use all threads for testing
  device: cuda
  use_mixed_precision: false  # Disable for debugging
  num_threads: 12
  
  # TensorRT acceleration (enabled by default for RTX GPUs)
  use_tensorrt: true
  tensorrt_fp16: true
  tensorrt_fallback: true
  tensorrt_workspace_size: 2048  # MB - workspace size for optimization
  tensorrt_int8: false           # INT8 quantization (requires calibration)
  tensorrt_max_batch_size: 512   # Maximum batch size to optimize for
  tensorrt_engine_cache_dir: null # Custom cache directory

network:
  # Tiny network for testing
  model_type: resnet
  input_channels: 20
  num_res_blocks: 2
  num_filters: 32
  value_head_hidden_size: 64

training:
  # Minimal training
  batch_size: 64
  learning_rate: 0.01
  num_epochs: 2
  
  # Few self-play games - use 4 workers for faster testing
  num_games_per_iteration: 12
  num_workers: 4
  max_moves_per_game: 81
  
  # Small buffer
  window_size: 1000
  augment_data: false
  mixed_precision: false
  dataloader_workers: 2
  pin_memory: true

arena:
  # Quick evaluation - parallel for speed
  num_games: 12
  num_workers: 4
  # games_per_worker: auto-calculated based on num_games / num_workers
  
  # ELO tracking
  elo_initial_rating: 0.0  # Standard ELO starting rating
  # games_per_worker: auto-calculated based on num_games / num_workers
  win_threshold: 0.5
  
  # Fast games
  temperature: 0.1
  mcts_simulations: 50
  
  # Skip random evaluation
  # min_win_rate_vs_random: 0.5  # DEPRECATED - Now using dynamic logarithmic scheduling
  # Dynamic threshold: starts at 5%, reaches 100% at iteration 150
  
  # Minimal logging
  save_game_records: false
  save_arena_logs: false