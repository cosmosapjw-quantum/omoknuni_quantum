# Gomoku Classical Configuration - Master/Expert Level
# Optimized for tournament-strength Gomoku AI

experiment_name: gomoku_master
seed: 42
log_level: INFO
num_iterations: 1000  # Increased for master-level strength

game:
  game_type: gomoku
  board_size: 15
  gomoku_use_renju: false
  gomoku_use_omok: false
  gomoku_use_pro_long_opening: false

mcts:
  num_simulations: 1000   # Increased for master-level tactical depth
  c_puct: 1.4
  dirichlet_alpha: 0.2
  dirichlet_epsilon: 0.4
  temperature: 1.5
  temperature_threshold: 50
  temperature_final: 0.5
  
  # Performance optimizatio
  min_wave_size: 3072      # Optimal for 80k+ sims/sec
  max_wave_size: 3072      # Fixed size for best performance
  adaptive_wave_sizing: false  # CRITICAL: Must be false
  batch_size: 1024 
  virtual_loss: 0.5
  
  # Memory and optimization - Adjusted for larger network
  memory_pool_size_mb: 3072   # Increased for larger network capacity
  max_tree_nodes: 750000      # Increased for deeper search trees
  tree_reuse: false
  tree_reuse_fraction: 0.5
  
  # GPU optimization - RTX 3060 Ti
  use_cuda_graphs: true
  use_mixed_precision: true   # FP16 for tensor cores
  use_tensor_cores: true      # Utilize 4864 CUDA cores
  compile_mode: reduce-overhead
  
  # Quantum features - disabled for classical
  quantum_level: classical
  enable_quantum: false
  
  # Quantum physics parameters (unused when classical)
  quantum_coupling: 0.1
  quantum_temperature: 1.0
  decoherence_rate: 0.01
  measurement_noise: 0.0
  
  # Path integral parameters (unused when classical)
  path_integral_steps: 10
  path_integral_beta: 1.0
  use_wick_rotation: true
  
  # Interference parameters (unused when classical)
  interference_alpha: 0.05
  interference_method: minhash
  minhash_size: 64
  phase_kick_strength: 0.1
  
  
  # CSR Tree parameters
  csr_max_actions: 225
  csr_use_sparse_operations: true
  
  # Interference thresholds
  interference_threshold: 0.1
  constructive_interference_factor: 0.1
  destructive_interference_factor: 0.05
  
  # Device configuration
  device: cuda
  num_threads: 22

network:
  # Optimized network for master-level Gomoku (no attack/defense overhead)
  model_type: resnet
  input_channels: 18         # Standard AlphaZero representation (removed attack/defense)
  input_representation: basic  # 'basic' (18ch), 'enhanced' (20ch), or 'standard' (3ch)
  num_res_blocks: 15         # Increased for deeper tactical understanding
  num_filters: 192           # Increased capacity for complex pattern recognition
  value_head_hidden_size: 512 # Larger value head for precise position evaluation
  policy_head_filters: 4     # Increased for better move prediction
  
  # Regularization - tuned for larger network
  dropout_rate: 0.15         # Slightly increased to prevent overfitting
  batch_norm: true
  batch_norm_momentum: 0.995 # Slower moving average for stability
  l2_regularization: 0.0002  # Increased regularization
  
  # Activation
  activation: relu
  weight_init: he_normal

training:
  # Training parameters optimized for master-level learning
  batch_size: 1024          # Reduced for better gradient estimates with larger network
  learning_rate: 0.01       # Reduced for more stable training with larger network
  learning_rate_schedule: cosine  # Smooth decay for long training
  lr_decay_steps: 100       # Longer cycles for deeper learning
  lr_decay_rate: 0.1
  min_learning_rate: 0.000001  # Lower minimum for fine-tuning
  
  # Optimization
  optimizer: adam
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  sgd_momentum: 0.9
  sgd_nesterov: true
  weight_decay: 0.0001
  
  # Gradient handling
  gradient_accumulation_steps: 2
  max_grad_norm: 5.0
  gradient_clip_value: null
  
  # Tournament settings for final evaluation
  final_tournament_enabled: true
  final_tournament_model_selection_step: 10
  final_tournament_max_models: 10
  
  # Training loop
  num_epochs: 7
  early_stopping_patience: 20
  early_stopping_min_delta: 0.0001
  
  # Self-play - Optimized for master-level training
  num_games_per_iteration: 120  # Increased for more diverse training data
  num_workers: 12  # Match CPU cores for optimal utilization
  max_moves_per_game: 225
  resign_threshold: -0.98      # More conservative resignation for master play
  resign_check_moves: 15       # Longer evaluation before resignation
  resign_start_iteration: 20   # Later resignation start for better early learning
  
  # Data handling - Optimized for master-level training
  window_size: 25000        # Smaller window for faster adaptation to diverse play
  sample_weight_by_game_length: true
  augment_data: true
  shuffle_buffer_size: 15000  # Increased buffer size
  dataloader_workers: 16     # Optimized for larger batches
  pin_memory: true
  persistent_workers: true
  
  # Mixed precision
  mixed_precision: true
  amp_opt_level: O1
  loss_scale: dynamic
  static_loss_scale: 1.0
  
  # Paths
  save_dir: checkpoints
  tensorboard_dir: runs
  data_dir: self_play_data

arena:
  # Battle settings - Rigorous evaluation for master level
  num_games: 100            # More games for statistical significance
  num_workers: 12
  win_threshold: 0.6        # Higher threshold for model acceptance
  statistical_significance: true
  confidence_level: 0.99    # Higher confidence level
  
  # Game settings - Tournament strength  
  temperature: 0.1          # Slightly stochastic for evaluation
  mcts_simulations: 400     # Match training simulation count
  c_puct: 1.414            # Match training c_puct
  max_moves: 225
  time_limit_seconds: null
  randomize_start_player: true
  
  # ELO settings
  elo_k_factor: 32.0
  elo_initial_rating: 0.0
  elo_anchor_rating: 0.0
  update_elo: true
  
  # Enhanced ELO features
  elo_enable_deflation: true        # Enable rating deflation to prevent inflation
  elo_deflation_factor: 0.99        # Deflation factor applied when needed
  elo_uncertainty_tracking: true     # Track rating uncertainty/confidence
  elo_validation_enabled: true      # Enable validation metrics
  elo_health_monitoring: true       # Enable health monitoring and inflation detection
  
  # Random policy evaluation - Master level expectation
  min_win_rate_vs_random: 0.99  # Near-perfect performance against random
  
  # Tournament settings
  tournament_rounds: 1
  tournament_games_per_pair: 10
  
  # Data saving
  save_game_records: true
  save_arena_logs: true
  arena_log_dir: arena_logs
  elo_save_path: elo_ratings.json
  
  # New arena features
  enable_current_vs_previous: true  # Enable current vs previous model arena matches
  enable_adaptive_random_matches: true  # Use adaptive logic for random matches
  enable_elo_consistency_checks: true  # Check for ELO inconsistencies
  enable_elo_auto_adjustment: true  # Automatically adjust ELO when inconsistencies are detected