# Go 19x19 Quantum Configuration
# Professional-level Go training with quantum enhancements

experiment_name: go_19x19_quantum
seed: 42
log_level: INFO
num_iterations: 1000

game:
  game_type: go
  board_size: 19
  go_komi: 7.5
  go_rules: chinese
  go_superko: true

mcts:
  # Core MCTS parameters - stronger for Go
  num_simulations: 1600
  c_puct: 1.0
  dirichlet_alpha: 0.03  # Much lower for 19x19 board
  dirichlet_epsilon: 0.25
  temperature: 1.0
  temperature_threshold: 30
  temperature_final: 0.1
  
  # Performance optimization - RTX 3060 Ti for large board
  min_wave_size: 1024  # Lower for 19x19 memory requirements
  max_wave_size: 2048  # Reduced to fit Go's larger state
  batch_size: 256  # Smaller batches for 19x19
  
  # Virtual loss - disabled for quantum versions (quantum interference handles diversity)
  virtual_loss: 0.0
  enable_virtual_loss: false
  
  # Larger memory for Go - 64GB RAM advantage
  memory_pool_size_mb: 8192  # Double for Go
  max_tree_nodes: 2000000  # Much larger tree
  tree_reuse: true
  
  # GPU optimization
  use_cuda_graphs: true
  use_mixed_precision: true
  use_tensor_cores: true
  
  # One-loop quantum for complex strategic game
  quantum_level: one_loop
  enable_quantum: true
  quantum_version: 'v2'
  quantum_config:
    quantum_coupling: 0.15
    quantum_temperature: 2.0
    decoherence_rate: 0.02
    measurement_noise: 0.002
    # Path integral for long-range planning
    path_integral_steps: 30
    path_integral_beta: 1.5
    use_wick_rotation: true
    # Interference for joseki diversity
    interference_alpha: 0.1
    interference_method: minhash
    minhash_size: 256
    phase_kick_strength: 0.2
  
  # Progressive widening parameters for lazy expansion
  progressive_widening_alpha: 0.5  # k = alpha * sqrt(n) - controls expansion rate
  progressive_widening_base: 10.0  # Minimum children to expand at each node
  
  # Device - Ryzen 9 5900X
  device: cuda
  num_threads: 24
  
  # TensorRT acceleration (enabled by default for RTX GPUs)
  use_tensorrt: true
  tensorrt_fp16: true
  tensorrt_fallback: true
  tensorrt_workspace_size: 2048  # MB - workspace size for optimization
  tensorrt_int8: false           # INT8 quantization (requires calibration)
  tensorrt_max_batch_size: 512   # Maximum batch size to optimize for
  tensorrt_engine_cache_dir: null # Custom cache directory

network:
  # Deeper network for Go complexity
  model_type: resnet
  input_channels: 20
  num_res_blocks: 20
  num_filters: 256
  value_head_hidden_size: 512
  policy_head_filters: 2
  
  # Regularization
  dropout_rate: 0.1
  batch_norm: true
  batch_norm_momentum: 0.997
  l2_regularization: 0.0001
  
  # Activation
  activation: relu
  weight_init: he_normal

training:
  # Training parameters adjusted for board size
  batch_size: 384  # Optimized for RTX 3060 Ti with 19x19
  learning_rate: 0.01
  learning_rate_schedule: cosine
  lr_decay_steps: 200
  lr_decay_rate: 0.1
  min_learning_rate: 0.00001
  
  # Optimizer
  optimizer: adam
  adam_beta1: 0.9
  adam_beta2: 0.999
  weight_decay: 0.0001
  
  # Gradient handling
  gradient_accumulation_steps: 2  # For effective batch size 768
  max_grad_norm: 5.0
  
  # Training loop
  num_epochs: 10
  
  # More self-play for Go - 12 cores
  num_games_per_iteration: 360  # 30 per worker
  num_workers: 12
  max_moves_per_game: 400
  resign_threshold: -0.95
  resign_check_moves: 20  # Check later in Go
  resign_start_iteration: 20  # Go is complex, needs more iterations to learn life/death
  
  # Data handling - 64GB RAM
  window_size: 1000000  # Much larger for Go
  augment_data: true
  mixed_precision: true
  dataloader_workers: 8
  pin_memory: true
  
  # Paths
  save_dir: checkpoints
  tensorboard_dir: runs
  data_dir: self_play_data

arena:
  # Stronger evaluation for Go - 12 cores
  num_games: 240
  num_workers: 12
  # games_per_worker: auto-calculated based on num_games / num_workers
  win_threshold: 0.55
  statistical_significance: true
  confidence_level: 0.95
  
  # Evaluation settings
  temperature: 0.1
  mcts_simulations: 800
  c_puct: 1.0
  max_moves: 400
  
  # ELO tracking
  elo_k_factor: 24.0  # More stable for Go
  elo_initial_rating: 0.0  # Standard ELO starting rating
  elo_anchor_rating: 0.0
  update_elo: true
  
  # Random policy checks
  min_win_rate_vs_random: 0.99  # Should dominate random
  
  # Tournament settings
  tournament_rounds: 1
  tournament_games_per_pair: 20
  
  # Logging
  save_game_records: false
  save_arena_logs: true
  arena_log_dir: arena_logs
  elo_save_path: elo_ratings.json