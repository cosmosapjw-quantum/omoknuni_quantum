# Chess Classical Configuration
# Professional chess training with established AlphaZero approach

experiment_name: chess_alphazero_classical
seed: 42
log_level: INFO
num_iterations: 800

game:
  game_type: chess
  chess_960: false
  chess_starting_fen: null

mcts:
  # Core MCTS parameters for chess
  num_simulations: 800
  c_puct: 1.0
  dirichlet_alpha: 0.3
  dirichlet_epsilon: 0.25
  temperature: 1.0
  temperature_threshold: 15  # Earlier transition in chess
  temperature_final: 0.1
  
  # Performance optimization - RTX 3060 Ti
  min_wave_size: 2048
  max_wave_size: 3072
  batch_size: 384
  
  # Virtual loss for leaf parallelization
  virtual_loss: 3.0
  enable_virtual_loss: true
  
  # Memory settings - 8GB VRAM + 64GB RAM
  memory_pool_size_mb: 4096
  max_tree_nodes: 1000000
  tree_reuse: true
  tree_reuse_fraction: 0.5
  
  # GPU optimization
  use_cuda_graphs: true
  use_mixed_precision: true
  use_tensor_cores: true
  compile_mode: reduce-overhead
  
  # Classical MCTS for chess (proven approach)
  enable_quantum: false
  
  # Progressive widening parameters for lazy expansion
  enable_progressive_widening: true
  progressive_widening_alpha: 0.5  # k = alpha * sqrt(n) - controls expansion rate
  progressive_widening_base: 10.0  # Minimum children to expand at each node
  
  # RAVE (Rapid Action Value Estimation) for faster learning
  enable_rave: true
  rave_c: 3000.0  # RAVE exploration constant
  rave_threshold: 50  # Disable RAVE after this many training iterations
  
  # Tactical boost for finding important moves with untrained networks
  enable_tactical_boost: true
  tactical_boost_strength: 0.3      # 0.0 = no boost, 1.0 = full replacement
  tactical_boost_decay: 0.99        # Decay factor per iteration
  
  # Chess tactical parameters  
  chess_capture_good_base: 10.0     # Base good capture bonus
  chess_capture_equal: 5.0          # Equal trade bonus
  chess_capture_bad: 2.0            # Bad capture bonus
  chess_check_boost: 8.0            # Check move bonus
  chess_check_capture_boost: 4.0    # Check + capture bonus
  chess_promotion_boost: 9.0        # Pawn promotion bonus
  chess_center_boost: 1.0           # Central control bonus
  chess_center_core_boost: 0.5      # Center core bonus
  chess_development_boost: 2.0      # Piece development bonus
  chess_castling_boost: 6.0         # Castling bonus
  
  # CSR tree configuration for Chess
  csr_max_actions: 4096  # Buffer for all possible chess moves including promotions
  
  # Device - Ryzen 9 5900X
  device: cuda
  num_threads: 24
  
  # TensorRT acceleration (enabled by default for RTX GPUs)
  use_tensorrt: true
  tensorrt_fp16: true
  tensorrt_fallback: true
  tensorrt_workspace_size: 2048  # MB - workspace size for optimization
  tensorrt_int8: false           # INT8 quantization (requires calibration)
  tensorrt_max_batch_size: 512   # Maximum batch size to optimize for
  tensorrt_engine_cache_dir: null # Custom cache directory

network:
  # Standard chess network architecture
  model_type: resnet
  input_channels: 18           # Standard AlphaZero representation
  input_representation: basic  # 'basic' (18ch), 'minimal' (3ch), or 'enhanced' (20ch)
  num_res_blocks: 20
  num_filters: 256
  value_head_hidden_size: 256
  policy_head_filters: 2
  
  # Regularization
  dropout_rate: 0.1
  batch_norm: true
  batch_norm_momentum: 0.997
  l2_regularization: 0.0001
  
  # Activation
  activation: relu
  weight_init: he_normal

training:
  # Training parameters
  batch_size: 768  # RTX 3060 Ti optimal
  learning_rate: 0.01
  learning_rate_schedule: step
  lr_decay_steps: 100
  lr_decay_rate: 0.1
  min_learning_rate: 0.00001
  
  # Optimizer
  optimizer: adam
  adam_beta1: 0.9
  adam_beta2: 0.999
  weight_decay: 0.0001
  
  # Gradient handling
  gradient_accumulation_steps: 1
  max_grad_norm: 5.0
  
  # Training loop
  num_epochs: 10
  
  # Self-play settings - 12 cores
  num_games_per_iteration: 300  # 25 per worker
  num_workers: 12
  max_moves_per_game: 500
  resign_threshold: -0.95
  resign_check_moves: 10
  resign_start_iteration: 15  # Chess needs more iterations to learn endgames
  
  # Data handling - 64GB RAM
  window_size: 750000  # Much larger
  augment_data: false  # No augmentation for chess
  mixed_precision: true
  dataloader_workers: 8
  pin_memory: true
  
  # Paths
  save_dir: checkpoints
  tensorboard_dir: runs
  data_dir: self_play_data

arena:
  # Evaluation settings - 12 cores
  num_games: 120
  num_workers: 12
  # games_per_worker: auto-calculated based on num_games / num_workers
  win_threshold: 0.55
  
  # Game settings
  temperature: 0.1
  mcts_simulations: 800  # Match training simulations for accurate evaluation
  c_puct: 1.0
  max_moves: 500
  
  # ELO tracking
  elo_k_factor: 32.0
  elo_initial_rating: 0.0  # Standard ELO starting rating
  elo_anchor_rating: 0.0
  update_elo: true
  
  # Random policy checks
  # min_win_rate_vs_random: 0.99  # DEPRECATED - Now using dynamic logarithmic scheduling
  # Dynamic threshold: starts at 5%, reaches 100% at iteration 150
  
  # Tournament settings
  tournament_rounds: 1
  tournament_games_per_pair: 10
  
  # Logging
  save_game_records: true  # Save chess games
  save_arena_logs: true
  arena_log_dir: arena_logs
  elo_save_path: elo_ratings.json