# Optimized Gomoku Training Configuration for 3-4 Day Budget
# Hardware: Ryzen 9 5900X (24 threads), 64GB RAM, RTX 3060 Ti (8GB VRAM)
# Target: Maximum learning progress within time/compute constraints

experiment_name: gomoku_intensive
seed: 42
log_level: INFO
num_iterations: 200  # Increased for 3-4 day training (est. 1 iter/hour = 72-96 total)

game:
  game_type: gomoku
  board_size: 15
  gomoku_use_renju: false            # Pure freestyle Gomoku
  gomoku_use_omok: false             # Pure freestyle Gomoku
  gomoku_use_pro_long_opening: false # No opening restrictions

mcts:
  # Optimized for RTX 3060 Ti performance and quality balance
  num_simulations: 800       # Increased for better play quality (within GPU limits)
  c_puct: 1.4                # Balanced exploration-exploitation
  dirichlet_alpha: 0.3       # Standard for Gomoku
  dirichlet_epsilon: 0.25    # Good noise level
  temperature: 1.0
  temperature_threshold: 30   # Extended exploration for stronger play
  temperature_final: 0.05    # Nearly deterministic endgame
  
  # RTX 3060 Ti optimized performance (8GB VRAM with multi-worker constraints)
  min_wave_size: 3072        # Reduced to leave memory for multiple workers
  max_wave_size: 3072        # Conservative to avoid OOM with 8 workers
  batch_size: 512            # Optimal for TensorRT + 8GB VRAM
  virtual_loss: 0.5          # Parallel exploration
  enable_virtual_loss: true
  
  # Enhanced tree reuse for efficiency - RE-ENABLED after root mapping fixes
  tree_reuse: true          # Critical for training speed
  
  # Memory optimization for 64GB RAM + 8GB VRAM (multi-worker constrained)
  memory_pool_size_mb: 4096  # Reduced to fit 8 workers in GPU memory
  max_tree_nodes: 600000     # Optimized for 8GB VRAM with 8 workers (~600MB per worker)
  
  # Maximum GPU optimization for RTX 3060 Ti
  use_cuda_graphs: true
  use_mixed_precision: true
  use_tensor_cores: true     # Leverage Ampere architecture
  
  # Performance mode
  enable_quantum: false
  classical_only_mode: true     # Optimize for performance
  enable_fast_ucb: true
  
  # Ryzen 9 5900X optimization (24 threads available)
  device: cuda
  num_threads: 22            # Increased since workers now use CPU efficiently
  
  # Production mode - disable debug overhead
  enable_debug_logging: false
  enable_state_pool_debug: false
  
  # Progressive widening parameters for lazy expansion
  enable_progressive_widening: true
  progressive_widening_alpha: 0.5  # k = alpha * sqrt(n) - controls expansion rate
  progressive_widening_base: 10.0  # Minimum children to expand at each node
  
  # RAVE (Rapid Action Value Estimation) for faster learning
  enable_rave: true
  rave_c: 3000.0  # RAVE exploration constant
  rave_threshold: 50  # Disable RAVE after this many training iterations
  
  # Tactical boost for finding important moves with untrained networks
  enable_tactical_boost: true
  tactical_boost_strength: 0.3      # 0.0 = no boost, 1.0 = full replacement
  tactical_boost_decay: 0.99        # Decay factor per iteration
  
  # Gomoku tactical parameters
  gomoku_win_boost: 100.0           # Immediate win move
  gomoku_block_win_boost: 90.0      # Block opponent win
  gomoku_open_four_boost: 50.0      # Create open four
  gomoku_block_four_boost: 45.0     # Block opponent open four
  gomoku_threat_base_boost: 40.0    # Base for multiple threats
  gomoku_threat_multiplier: 5.0     # Per additional threat
  gomoku_three_boost: 20.0          # Create three in a row
  gomoku_block_three_boost: 18.0    # Block opponent three
  gomoku_center_boost: 3.0          # Central position bonus
  gomoku_connection_boost: 2.0      # Per connection bonus
  
  # TensorRT optimizations for RTX 3060 Ti
  use_tensorrt: true
  tensorrt_fp16: true
  tensorrt_fallback: true
  tensorrt_workspace_size: 2048  # 2GB workspace for complex optimizations

network:
  # Optimized ResNet for RTX 3060 Ti (maximize learning within VRAM constraints)
  model_type: resnet
  input_channels: 18
  input_representation: basic
  num_res_blocks: 12          # Increased for better pattern recognition
  num_filters: 128            # Slightly larger for better capacity (still efficient)
  value_head_hidden_size: 256 # Enlarged for better value estimation
  policy_head_filters: 2
  
  # Optimized regularization for 3-4 day training
  dropout_rate: 0.15         # Increased to prevent overfitting with longer training
  batch_norm: true
  batch_norm_momentum: 0.995 # Slightly higher momentum for stability
  l2_regularization: 0.0003  # Increased regularization
  
  # Activation optimized for Tensor Cores
  activation: swish          # Better convergence than ReLU
  weight_init: he_normal

training:
  # Optimized for 3-4 day intensive training
  batch_size: 1024            # Larger batches for better GPU utilization (64GB RAM allows this)
  learning_rate: 0.002       # Slightly higher for faster initial learning
  learning_rate_schedule: cosine_with_restarts  # Better for long training
  lr_warmup_steps: 2000      # Longer warmup for stability
  lr_decay_steps: 40         # More frequent decay steps
  lr_decay_rate: 0.85        # Slightly more aggressive decay
  min_learning_rate: 0.00005 # Lower floor for fine-tuning
  
  # AdamW optimizer for better long-term training
  optimizer: adamw
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  weight_decay: 0.01         # Higher weight decay for longer training
  
  # Gradient handling optimized for stability
  gradient_accumulation_steps: 2  # Effective batch size = 768
  max_grad_norm: 3.0         # Tighter clipping for stability
  gradient_clip_value: null
  
  # Training loop optimized for efficiency
  num_epochs: 10               # More epochs for thorough learning
  early_stopping_patience: 15  # More patience for longer training
  early_stopping_min_delta: 0.0003
  
  # Self-play settings optimized for CPU workers (no more VRAM constraints)
  num_games_per_iteration: 120  # Adjusted for optimal worker count (12 workers × 10 games each)
  num_workers: 12            # Optimal: match physical cores (12) for CPU-intensive MCTS
  max_moves_per_game: 225    # Full board
  resign_threshold: -0.92    # Slightly more conservative
  resign_check_moves: 18     # More moves to verify resignation
  resign_start_iteration: 15 # Start resignation later for better early data
  
  # Larger replay buffer for 3-4 day training (64GB RAM advantage)
  window_size: 50000         # Much larger for better sample diversity
  sample_weight_by_game_length: true
  augment_data: true
  shuffle_buffer_size: 15000 # Larger shuffle buffer
  dataloader_workers: 12     # Increased for faster data loading during NN training (separate from self-play)
  pin_memory: true
  persistent_workers: true
  
  # Mixed precision optimized for Ampere architecture
  mixed_precision: true
  amp_opt_level: O2          # More aggressive mixed precision
  loss_scale: dynamic
  
  # Paths for 3-day intensive training
  save_dir: experiments/gomoku_3day_intensive/checkpoints
  tensorboard_dir: experiments/gomoku_3day_intensive/runs
  data_dir: experiments/gomoku_3day_intensive/self_play_data

arena:
  # Optimized evaluation for intensive 3-4 day training
  num_games: 40              # More games for better statistical confidence
  num_workers: 12            # More workers for faster evaluation (leverage CPU)
  win_threshold: 0.53        # Slightly more stringent for quality control
  statistical_significance: true
  confidence_level: 0.95
  
  # Game settings - match training configuration
  temperature: 0.0           # Deterministic for evaluation
  mcts_simulations: 100      # Match training simulations exactly
  c_puct: 1.4                # Match training c_puct
  max_moves: 225
  time_limit_seconds: null
  randomize_start_player: true
  
  # ELO settings optimized for frequent evaluation
  elo_k_factor: 32.0  # Standard K-factor for ELO updates
  elo_initial_rating: 0.0
  elo_anchor_rating: 0.0
  update_elo: true
  
  # Enhanced ELO tracking for long training
  elo_enable_deflation: true
  elo_uncertainty_tracking: true
  elo_validation_enabled: true
  elo_health_monitoring: true
  
  # Tournament settings
  tournament_rounds: 1
  tournament_games_per_pair: 24  # More games per pair for better evaluation
  
  # Data saving with updated paths
  save_game_records: true
  save_arena_logs: true
  arena_log_dir: experiments/gomoku_3day_intensive/arena_logs
  elo_save_path: experiments/gomoku_3day_intensive/elo_ratings.json

# 3-4 Day Intensive Training Strategy (8GB VRAM Optimized):
# 
# Hardware Optimizations for Ryzen 9 5900X + RTX 3060 Ti + 64GB RAM:
# 1. MCTS simulations: set to 100 (balanced for quality and speed)
# 2. Wave size: 2048 → 2048 (balanced for multi-worker GPU memory)
# 3. Workers: 16 → 8 (GPU memory constraint: 8 × ~600MB = ~4.8GB)
# 4. Batch size: 256 → 384 (leverage abundant RAM)
# 5. Memory pool: 4GB per worker (optimized for 8GB total VRAM)
# 6. ResNet blocks: 10 → 12 (stronger pattern recognition)
# 7. Window size: 30K → 50K (much larger replay buffer)
# 8. Iterations: 100 → 200 (3-4 day training capacity)
#
# Resource Allocation (RTX 3060 Ti - 8GB + Ryzen 9 5900X 12 cores/24 threads):
# - GPU: TensorRT model (~1.5GB) + evaluation service
# - CPU: 12 MCTS workers (optimal: 1 per physical core) + 12 dataloader workers
# - RAM: 12 × ~500MB per worker tree = ~6GB (out of 64GB available)
# - Total GPU usage: ~2GB (well within 8GB capacity)
#
# Performance Estimates:
# - ~1-1.5 iterations/hour (with optimizations)
# - 72-96 hours = 72-144 total iterations
# - Expected final strength: 1400-1800 ELO
# - Memory usage: ~8GB GPU, ~25-35GB RAM
#
# Training Timeline:
# Day 1: Iterations 1-30 (learning basic patterns)
# Day 2: Iterations 31-80 (tactical improvements)  
# Day 3: Iterations 81-130 (strategic refinement)
# Day 4: Iterations 131-200 (polish and convergence)