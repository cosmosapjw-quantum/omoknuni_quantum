# Optimized Gomoku Training Configuration for 3-4 Day Budget
# Hardware: Ryzen 9 5900X (24 threads), 64GB RAM, RTX 3060 Ti (8GB VRAM)
# Target: Maximum learning progress within time/compute constraints

experiment_name: gomoku_intensive
seed: 42
log_level: INFO
num_iterations: 200  # Increased for 3-4 day training (est. 1 iter/hour = 72-96 total)

game:
  game_type: gomoku
  board_size: 15
  # Game settings use default Gomoku rules

mcts:
  # Optimized for RTX 3060 Ti performance and quality balance
  num_simulations: 500       # Increased for better exploration and play quality
  c_puct: 1.4                # Balanced exploration-exploitation
  dirichlet_alpha: 0.3       # Standard for Gomoku
  dirichlet_epsilon: 0.25    # Reasonable exploration noise for training
  temperature: 1.0
  temperature_threshold: 30   # Extended exploration for stronger play
  # temperature_final is calculated from temperature at runtime
  
  # RTX 3060 Ti optimized performance based on Optuna results
  min_wave_size: 4096         # Optimal minimum from optimization
  max_wave_size: 16384        # Best performance with larger waves
  batch_size: 2048            # Optimal batch size from trials
  virtual_loss: 1.0           # Standard virtual loss
  enable_virtual_loss: true
  
  # Tree reuse for better performance
  enable_subtree_reuse: true
  
  # Memory optimization for 64GB RAM + 8GB VRAM (optimized values)
  memory_pool_size_mb: 4096  # Optimal from optimization (less is more)
  max_tree_nodes: 1000000     # Balanced for performance
  
  # Maximum GPU optimization for RTX 3060 Ti
  # GPU optimizations handled automatically
  
  # Performance mode
  enable_fast_ucb: true
  
  # CSR tree configuration for Gomoku
  csr_max_actions: 225       # Full board size for 15x15 Gomoku
  
  # Ryzen 9 5900X optimization (24 threads available)
  device: cuda
  num_threads: 22            # CPU threads for MCTS tree operations
  
  # Production mode - disable debug overhead
  enable_debug_logging: false
  enable_state_pool_debug: false
  
  # Tactical boost for finding important moves with untrained networks
  enable_tactical_boost: false
  tactical_boost_strength: 0.3      # 0.0 = no boost, 1.0 = full replacement
  tactical_boost_decay: 0.99        # Decay factor per iteration
  
  # TensorRT settings moved to training section

network:
  # Optimized ResNet for RTX 3060 Ti (maximize learning within VRAM constraints)
  model_type: resnet
  input_channels: 19
  input_representation: basic
  num_res_blocks: 9           # Increased for better pattern recognition
  num_filters: 64             # Slightly larger for better capacity (still efficient)
  value_head_hidden_size: 128 # Enlarged for better value estimation
  policy_head_filters: 2
  
  # Optimized regularization for 3-4 day training
  dropout_rate: 0.15         # Increased to prevent overfitting with longer training
  batch_norm: true
  batch_norm_momentum: 0.995 # Slightly higher momentum for stability
  l2_regularization: 0.0003  # Increased regularization
  
  # Activation optimized for Tensor Cores
  activation: gelu           # Better convergence than ReLU
  weight_init: he_normal

training:
  # Optimized for 3-4 day intensive training
  batch_size: 1024            # Larger batches for better GPU utilization (64GB RAM allows this)
  learning_rate: 0.0005       # Reduced to prevent training collapse
  learning_rate_schedule: cosine  # Better for long training
  lr_decay_steps: 40         # More frequent decay steps
  lr_decay_rate: 0.85        # Slightly more aggressive decay
  min_learning_rate: 0.00005 # Lower floor for fine-tuning
  
  # AdamW optimizer for better long-term training
  optimizer: adamw
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  weight_decay: 0.01         # Higher weight decay for longer training
  
  # Gradient handling optimized for stability
  gradient_accumulation_steps: 2  # Effective batch size = 768
  max_grad_norm: 3.0         # Tighter clipping for stability
  gradient_clip_value: null
  
  # Training loop optimized for efficiency
  num_epochs: 10               # More epochs for thorough learning
  early_stopping_patience: 15  # More patience for longer training
  early_stopping_min_delta: 0.0003
  
  # Optimized batch processing from Optuna results
  inference_batch_size: 1024    # Optimal inference batch size
  
  # Self-play settings optimized for single-GPU execution
  num_games_per_iteration: 100  # Sequential games on single GPU
  max_moves_per_game: 225    # Full board
  resign_threshold: -0.92    # Slightly more conservative
  resign_check_moves: 18     # More moves to verify resignation
  resign_start_iteration: 15 # Start resignation later for better early data
  
  # Larger replay buffer for 3-4 day training (64GB RAM advantage)
  window_size: 100000         # Much larger for better sample diversity
  sample_weight_by_game_length: true
  augment_data: true
  shuffle_buffer_size: 20000 # Larger shuffle buffer
  dataloader_workers: 16     # DataLoader workers for training (not self-play)
  pin_memory: true
  # Persistent workers not needed for single-GPU
  
  # Q-value training for stable learning
  use_mcts_q_values: true  # Use MCTS Q-values as training targets
  q_value_weight: 0.6  # Mix 60% Q-values with 40% game outcomes
  q_value_temperature: 0.1
  
  # KL divergence regularization
  kl_weight: 0.3  # Moderate KL regularization to prevent collapse
  kl_warmup_iterations: 10  # Apply KL after 10 iterations
  
  # Mixed precision optimized for Ampere architecture
  mixed_precision: true
  # Mixed precision handled by PyTorch native AMP
  loss_scale: dynamic
  
  # Paths for 3-day intensive training
  save_dir: experiments/gomoku_intensive/checkpoints
  tensorboard_dir: experiments/gomoku_intensive/runs
  data_dir: self_play_data

arena:
  # Optimized evaluation for intensive 3-4 day training
  num_games: 40              # More games for better statistical confidence
  win_threshold: 0.53        # Slightly more stringent for quality control
  statistical_significance: true
  confidence_level: 0.95
  
  # Game settings - match training configuration
  temperature: 0.0           # Deterministic for evaluation
  mcts_simulations: 500      # Match training simulations exactly
  c_puct: 1.4                # Match training c_puct
  max_moves: 225
  time_limit_seconds: null
  randomize_start_player: true
  
  # ELO settings optimized for frequent evaluation
  elo_k_factor: 32.0  # Standard K-factor for ELO updates
  elo_initial_rating: 0.0
  elo_anchor_rating: 0.0
  update_elo: true
  
  # ELO tracking enabled by update_elo flag
  
  # Tournament settings
  tournament_rounds: 1
  tournament_games_per_pair: 24  # More games per pair for better evaluation
  
  # Data saving with updated paths
  save_game_records: true
  save_arena_logs: true
  arena_log_dir: arena_logs
  elo_save_path: elo_ratings.json

# 3-4 Day Intensive Training Strategy (8GB VRAM Optimized):
# 
# Hardware Optimizations for Ryzen 9 5900X + RTX 3060 Ti + 64GB RAM:
# 1. MCTS simulations: 400 (balanced for quality and speed)
# 2. Wave size: 512 â†’ 7936 (optimized based on Optuna results)
# 3. Single GPU: Direct GPU evaluation with optimal batching
# 4. Batch size: 768 (optimal from 4700+ sims/sec benchmark)
# 5. Memory pool: 2GB (optimal - less memory, better performance)
# 6. ResNet blocks: 12 (stronger pattern recognition)
# 7. Window size: 50K (much larger replay buffer)
# 8. Iterations: 200 (3-4 day training capacity)
#
# Resource Allocation (RTX 3060 Ti - 8GB + Ryzen 9 5900X 12 cores/24 threads):
# - GPU: TensorRT model (~1.5GB) + direct GPU evaluation + MCTS tree operations
# - CPU: 22 threads for parallel MCTS tree operations
# - RAM: Single large MCTS tree (optimized for memory efficiency)
# - Total GPU usage: ~4-5GB (well within 8GB capacity)
#
# Performance Estimates:
# - ~1-1.5 iterations/hour (with optimizations)
# - 72-96 hours = 72-144 total iterations
# - Expected final strength: 1400-1800 ELO
# - Memory usage: ~8GB GPU, ~25-35GB RAM
#
# Training Timeline:
# Day 1: Iterations 1-30 (learning basic patterns)
# Day 2: Iterations 31-80 (tactical improvements)  
# Day 3: Iterations 81-130 (strategic refinement)
# Day 4: Iterations 131-200 (polish and convergence)