# Tournament-Level Gomoku Configuration
# Target: Professional/Tournament strength AI (2200+ ELO)
# Hardware: RTX 3060 Ti (8GB VRAM), Ryzen 9 5900X (12c/24t), 64GB RAM
# Training time: Extended training over 2-3 months

experiment_name: gomoku_tournament_3060ti
seed: 42
log_level: INFO
num_iterations: 400  # Adjusted for realistic 2-3 month training

game:
  game_type: gomoku
  board_size: 15
  gomoku_use_renju: false            # Freestyle for maximum flexibility
  gomoku_use_omok: false             # Pure gomoku rules
  gomoku_use_pro_long_opening: false # No restrictions for strongest play

mcts:
  # Tournament-level search parameters (optimized for RTX 3060 Ti)
  num_simulations: 800       # Balanced for quality vs speed (267ms per move @ 3000 sims/sec)
  c_puct: 1.25               # Lower for more exploitation in strong play
  dirichlet_alpha: 0.15      # Lower noise for more focused search
  dirichlet_epsilon: 0.1     # Minimal exploration noise
  temperature: 1.0
  temperature_threshold: 20   # Shorter exploration phase
  temperature_final: 0.01    # Near-deterministic for precise play
  
  # RTX 3060 Ti optimized settings (8GB VRAM constraint)
  min_wave_size: 2048        # Optimized for 8GB VRAM with multiple workers
  max_wave_size: 2048        
  batch_size: 512            # Smaller batches to fit memory
  virtual_loss: 0.3          # Lower virtual loss for more accurate evaluation
  enable_virtual_loss: true
  
  # Tree reuse for opening book development
  tree_reuse: true           # Essential for tournament play
  
  # Memory settings optimized for 64GB RAM / 8GB VRAM
  memory_pool_size_mb: 8192   # 8GB - conservative for multi-worker stability
  max_tree_nodes: 1000000     # 1M nodes per worker (fits in VRAM)
  
  # GPU optimization
  use_cuda_graphs: true
  use_mixed_precision: true
  use_tensor_cores: true
  
  # Performance mode
  enable_quantum: false
  classical_only_mode: true
  enable_fast_ucb: true
  
  # CSR Tree parameters
  csr_max_actions: 225
  csr_use_sparse_operations: true
  
  # Progressive widening for selective search
  progressive_widening_alpha: 0.3  # More selective expansion
  progressive_widening_base: 15.0  # Higher base for quality moves
  
  # Device configuration (Ryzen 9 5900X)
  device: cuda
  num_threads: 22  # Leave 2 threads for system
  
  # TensorRT optimization (RTX 3060 Ti)
  use_tensorrt: true
  tensorrt_fp16: true
  tensorrt_fallback: true
  tensorrt_workspace_size: 2048  # 2GB workspace (conservative)
  tensorrt_int8: false           # Disable INT8 (requires calibration)
  tensorrt_max_batch_size: 512
  tensorrt_engine_cache_dir: experiments/gomoku_tournament/tensorrt_cache

network:
  # Optimized network for RTX 3060 Ti while maintaining tournament strength
  model_type: resnet
  input_channels: 18
  input_representation: basic
  num_res_blocks: 15          # Reduced from 20 (still very deep)
  num_filters: 192            # Reduced from 256 (fits 8GB VRAM)
  value_head_hidden_size: 384 # Reduced but still large
  policy_head_filters: 4      # Keep enhanced policy head
  
  # Regularization adjusted for network size
  dropout_rate: 0.18         # Slightly reduced
  batch_norm: true
  batch_norm_momentum: 0.997 # Keep slow momentum
  l2_regularization: 0.0004  # Adjusted regularization
  
  # Advanced activation
  activation: gelu           # GELU for better gradient flow
  weight_init: he_normal

training:
  # Tournament-level training (adjusted for hardware)
  batch_size: 768            # Fits in 8GB VRAM with model
  learning_rate: 0.0015      # Slightly higher for smaller batches
  learning_rate_schedule: cosine_with_restarts
  lr_warmup_steps: 3000      # Adjusted warmup
  lr_decay_steps: 40         # More frequent restarts
  lr_decay_rate: 0.9         # Keep gentle decay
  min_learning_rate: 0.00001 # Very low floor for fine-tuning
  
  # Advanced optimizer
  optimizer: adamw
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  weight_decay: 0.01
  
  # Gradient optimization
  gradient_accumulation_steps: 3  # Effective batch = 2304
  max_grad_norm: 1.0         # Tight gradient clipping
  gradient_clip_value: null
  
  # Extended training
  num_epochs: 15             # Balanced for iteration speed
  early_stopping_patience: 25 # Still patient
  early_stopping_min_delta: 0.0001
  
  # High-quality self-play (optimized for 12 CPU cores)
  num_games_per_iteration: 300  # Reduced but still substantial
  num_workers: 12            # Match physical CPU cores
  max_moves_per_game: 225
  resign_threshold: -0.98    # Slightly more aggressive
  resign_check_moves: 25     # Balanced resignation check
  resign_start_iteration: 40 # Adjusted for 400 iterations
  
  # Large replay buffer (utilizing 64GB RAM)
  window_size: 120000        # Still very large (fits in RAM)
  sample_weight_by_game_length: true
  augment_data: true
  shuffle_buffer_size: 30000 # Adjusted shuffle buffer
  dataloader_workers: 12     # Match workers
  pin_memory: true
  persistent_workers: true
  
  # Mixed precision
  mixed_precision: true
  amp_opt_level: O2
  loss_scale: dynamic
  
  # Tournament preparation features
  final_tournament_enabled: true
  final_tournament_model_selection_step: 20
  final_tournament_max_models: 20
  
  # Paths
  save_dir: experiments/gomoku_tournament/checkpoints
  tensorboard_dir: experiments/gomoku_tournament/runs
  data_dir: experiments/gomoku_tournament/self_play_data

arena:
  # Rigorous evaluation for tournament strength
  num_games: 100             # Balanced for iteration speed
  num_workers: 12            # Match CPU cores
  win_threshold: 0.52        # Small improvement threshold
  statistical_significance: true
  confidence_level: 0.98     # Slightly relaxed
  
  # Tournament match settings
  temperature: 0.0           # Deterministic play
  mcts_simulations: 400      # Balanced for evaluation speed
  c_puct: 1.25               # Match training
  max_moves: 225
  time_limit_seconds: null
  randomize_start_player: true
  
  # Professional ELO tracking
  elo_k_factor: 16.0         # Low K-factor for stable ratings
  elo_initial_rating: 1500.0 # Start at club level
  elo_anchor_rating: 1500.0
  update_elo: true
  
  # Advanced ELO features
  elo_enable_deflation: true
  elo_deflation_factor: 0.995
  elo_uncertainty_tracking: true
  elo_validation_enabled: true
  elo_health_monitoring: true
  
  # Tournament settings
  tournament_rounds: 3       # Multiple rounds
  tournament_games_per_pair: 50  # Many games per pairing
  
  # Data tracking
  save_game_records: true
  save_arena_logs: true
  arena_log_dir: experiments/gomoku_tournament/arena_logs
  elo_save_path: experiments/gomoku_tournament/elo_ratings.json
  
  # Advanced features
  enable_current_vs_previous: true
  enable_adaptive_random_matches: false  # No random matches at pro level
  enable_elo_consistency_checks: true
  enable_elo_auto_adjustment: true

# Tournament-Level Training Strategy (RTX 3060 Ti Optimized):
#
# Network Architecture (15 blocks, 192 filters):
# - Deep network maintaining ~85% of original capacity
# - Fits comfortably in 8GB VRAM with headroom
# - Still capable of learning complex patterns
#
# MCTS Configuration (800 simulations):
# - Strong tactical analysis (267ms per move)
# - Tree reuse for opening development
# - Progressive widening for quality
#
# Training Timeline (400 iterations):
# - 300 games/iteration = 120,000 total games
# - Estimated time per iteration: 2.5-3 hours
#   * Self-play: 300 games @ 13.4s/game = ~67 minutes
#   * Neural network training: ~60-80 minutes
#   * Arena evaluation: ~30-40 minutes
# - Total training time: 60-75 days (2-2.5 months)
#
# Expected Progression:
# - Iterations 1-40: Fundamentals (1600-1800 ELO)
# - Iterations 41-120: Advanced tactics (1900-2000 ELO)
# - Iterations 121-250: Strategic mastery (2000-2100 ELO)
# - Iterations 251-400: Tournament refinement (2100-2200+ ELO)
#
# Hardware Utilization:
# - GPU: ~6-7GB VRAM usage (comfortable margin)
# - RAM: ~40-50GB peak usage
# - Storage: ~300GB for full training data
# - CPU: 12 workers fully utilized
#
# Final Strength Expectations:
# - ELO: 2100-2200 (strong tournament level)
# - Professional amateur strength
# - Competitive in online tournaments
# - Strong opening repertoire
# - Solid endgame technique
#
# Note: While slightly below the 2200-2400 ELO of unlimited hardware,
# this configuration can still achieve tournament-competitive strength
# through patient, extended training.